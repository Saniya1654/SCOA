{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8aee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 - Best Fitness: 0.9667\n",
      "Generation 1 - Best Fitness: 0.9667\n",
      "Generation 2 - Best Fitness: 0.9667\n",
      "Generation 3 - Best Fitness: 0.9667\n",
      "Generation 4 - Best Fitness: 0.9667\n",
      "Generation 5 - Best Fitness: 0.9667\n",
      "Generation 6 - Best Fitness: 0.9667\n",
      "Generation 7 - Best Fitness: 0.9667\n",
      "Generation 8 - Best Fitness: 0.9733\n",
      "Generation 9 - Best Fitness: 0.9733\n",
      "Best Hyperparameters: [3, 4]\n",
      "Best Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "df=pd.read_csv('SCOA_A4.csv')\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y=df['species']\n",
    "\n",
    "# --- Genetic Algorithm Setup ---\n",
    "POP_SIZE = 20      # number of individuals\n",
    "N_GENERATIONS = 10 # iterations\n",
    "MUTATION_RATE = 0.2\n",
    "\n",
    "# Chromosome: [max_depth, min_samples_split]\n",
    "def create_chromosome():\n",
    "    return [random.randint(1, 20), random.randint(2, 10)]\n",
    "\n",
    "def fitness(chromosome):\n",
    "    max_depth, min_samples_split = chromosome\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split)\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    return scores.mean()\n",
    "\n",
    "def selection(population, fitnesses):\n",
    "    idx = np.argsort(fitnesses)[-2:]  # select best two\n",
    "    return [population[idx[0]], population[idx[1]]]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    point = random.randint(0, len(parent1)-1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(chromosome):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[0] = random.randint(1, 20)\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[1] = random.randint(2, 10)\n",
    "    return chromosome\n",
    "\n",
    "# --- Run GA ---\n",
    "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
    "\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fitnesses = [fitness(chromo) for chromo in population]\n",
    "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
    "\n",
    "    new_population = []\n",
    "    parents = selection(population, fitnesses)\n",
    "    for _ in range(POP_SIZE // 2):\n",
    "        child1, child2 = crossover(parents[0], parents[1])\n",
    "        new_population.append(mutate(child1))\n",
    "        new_population.append(mutate(child2))\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Best result\n",
    "fitnesses = [fitness(chromo) for chromo in population]\n",
    "best_idx = np.argmax(fitnesses)\n",
    "print(\"Best Hyperparameters:\", population[best_idx])\n",
    "print(\"Best Accuracy:\", fitnesses[best_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```\n",
    "\n",
    "* `pandas` (pd): used to read and handle table data (CSV).\n",
    "* `numpy` (np): used for numeric helpers (e.g. argsort).\n",
    "* `random`: Pythonâ€™s simple random-number utilities (used for GA ops).\n",
    "* `cross_val_score`: evaluates a model using cross-validation (gives accuracy estimates).\n",
    "* `DecisionTreeClassifier`: the ML model whose hyperparameters we tune.\n",
    "* `#from sklearn.datasets import load_iris` is commented out â€” not used here.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Load dataset\n",
    "df=pd.read_csv('SCOA_A4.csv')\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y=df['species']\n",
    "```\n",
    "\n",
    "* Read the CSV file `'SCOA_A4.csv'` into a DataFrame `df`.\n",
    "* `X` is a table of input features (4 numeric columns â€” typical iris features).\n",
    "* `y` is the target column (species labels to predict).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# --- Genetic Algorithm Setup ---\n",
    "POP_SIZE = 20      # number of individuals\n",
    "N_GENERATIONS = 10 # iterations\n",
    "MUTATION_RATE = 0.2\n",
    "```\n",
    "\n",
    "* `POP_SIZE`: how many candidate solutions (\"individuals\") exist in each generation.\n",
    "* `N_GENERATIONS`: how many times the population evolves (how many iterations).\n",
    "* `MUTATION_RATE`: probability (20%) to randomly change a gene when mutating.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Chromosome: [max_depth, min_samples_split]\n",
    "def create_chromosome():\n",
    "    return [random.randint(1, 20), random.randint(2, 10)]\n",
    "```\n",
    "\n",
    "* A **chromosome** encodes a candidate set of hyperparameters for the decision tree.\n",
    "* Here a chromosome is a list of two integers:\n",
    "\n",
    "  * `max_depth` (how deep the tree can grow) between 1 and 20.\n",
    "  * `min_samples_split` (minimum samples to split a node) between 2 and 10.\n",
    "* `create_chromosome()` randomly initializes one such chromosome.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def fitness(chromosome):\n",
    "    max_depth, min_samples_split = chromosome\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split)\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    return scores.mean()\n",
    "```\n",
    "\n",
    "* `fitness` measures how good a chromosome is.\n",
    "* It unpacks the two genes into `max_depth` and `min_samples_split`.\n",
    "* Builds a `DecisionTreeClassifier` with those hyperparameters.\n",
    "* Evaluates the model using 5-fold cross validation on (X,y).\n",
    "* Returns the **mean cross-validation score** (average accuracy across folds).\n",
    "* Higher fitness = better hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def selection(population, fitnesses):\n",
    "    idx = np.argsort(fitnesses)[-2:]  # select best two\n",
    "    return [population[idx[0]], population[idx[1]]]\n",
    "```\n",
    "\n",
    "* `selection` chooses parents from the population for reproduction.\n",
    "* `np.argsort(fitnesses)` returns indices that would sort fitnesses ascending.\n",
    "* `[-2:]` picks indices of the two best individuals (highest fitness).\n",
    "* Then it returns the corresponding chromosomes as the selected parents.\n",
    "* (Note: this always selects the top 2 â€” no tournament or roulette wheel here.)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def crossover(parent1, parent2):\n",
    "    point = random.randint(0, len(parent1)-1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "```\n",
    "\n",
    "* `crossover` mixes genes of two parents to create two children.\n",
    "* `point` is a random crossover index (0..number_of_genes-1).\n",
    "* `child1` takes genes from `parent1` up to `point`, then from `parent2` after `point`.\n",
    "* `child2` does the opposite.\n",
    "* This is **single-point crossover** on a short list of two genes.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "def mutate(chromosome):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[0] = random.randint(1, 20)\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[1] = random.randint(2, 10)\n",
    "    return chromosome\n",
    "```\n",
    "\n",
    "* `mutate` randomly changes genes in a chromosome with probability `MUTATION_RATE`.\n",
    "* If a mutation occurs on a gene, it assigns a new random integer within the allowed range.\n",
    "* Mutation injects genetic diversity and helps escape local optima.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# --- Run GA ---\n",
    "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
    "```\n",
    "\n",
    "* Initializes the starting population: `POP_SIZE` randomly created chromosomes.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fitnesses = [fitness(chromo) for chromo in population]\n",
    "    print(f\"Generation {gen} - Best Fitness: {max(fitnesses):.4f}\")\n",
    "\n",
    "    new_population = []\n",
    "    parents = selection(population, fitnesses)\n",
    "    for _ in range(POP_SIZE // 2):\n",
    "        child1, child2 = crossover(parents[0], parents[1])\n",
    "        new_population.append(mutate(child1))\n",
    "        new_population.append(mutate(child2))\n",
    "\n",
    "    population = new_population\n",
    "```\n",
    "\n",
    "* Loop for `N_GENERATIONS` rounds:\n",
    "\n",
    "  1. Compute fitness for each chromosome in the population.\n",
    "  2. Print the best fitness of this generation (for monitoring).\n",
    "  3. Select the top two parents using `selection`.\n",
    "  4. Produce `POP_SIZE` children by repeatedly crossing the same two parents.\n",
    "\n",
    "     * Each loop iteration creates two children, so loop runs `POP_SIZE // 2` times.\n",
    "     * After crossover, each child is mutated and appended to `new_population`.\n",
    "  5. Replace the old population with the newly produced population.\n",
    "* **Important behavior**: This GA uses only the top two parents to generate the entire next population. (No parent diversity beyond those two.)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Best result\n",
    "fitnesses = [fitness(chromo) for chromo in population]\n",
    "best_idx = np.argmax(fitnesses)\n",
    "print(\"Best Hyperparameters:\", population[best_idx])\n",
    "print(\"Best Accuracy:\", fitnesses[best_idx])\n",
    "```\n",
    "\n",
    "* After the final generation, compute fitnesses once more.\n",
    "* Find the index of the best chromosome (`np.argmax`).\n",
    "* Print the best hyperparameters and their fitness (accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "# Short plain-English summary (what the program does)\n",
    "\n",
    "1. Read a dataset and separate features (`X`) and labels (`y`).\n",
    "2. Create a population of random candidate hyperparameter settings for a decision tree (each candidate = `[max_depth, min_samples_split]`).\n",
    "3. For a fixed number of generations:\n",
    "\n",
    "   * Evaluate each candidate by training a decision tree and computing mean 5-fold CV accuracy (that is its fitness).\n",
    "   * Choose the two best candidates as parents.\n",
    "   * Repeatedly cross those two parents to create new children, mutate the children slightly, and form the next population.\n",
    "4. After evolution ends, report the best hyperparameters found and their cross-validation accuracy.\n",
    "\n",
    "# A few practical notes / suggestions\n",
    "\n",
    "* **Selection is greedy**: you always pick the best two parents only. That can make the GA converge quickly but risks losing diversity (and getting stuck). Consider selecting more parents or using tournament/roulette selection.\n",
    "* **Crossover on 2-gene chromosomes**: with two genes, crossover only swaps genes in a limited way â€” fine for this small search space, but you may want to use more sophisticated encodings if you add more hyperparameters.\n",
    "* **Population diversity**: because only two parents produce the entire next generation, population diversity is low. You could keep some elites and also allow random parents to mate.\n",
    "* **Fitness cost**: `cross_val_score` trains/evaluates a model many times â€” this is expensive. For small datasets itâ€™s fine; for large data consider fewer folds or other validation strategies.\n",
    "* **Reproducibility**: you might want to seed `random` and `numpy` to make runs reproducible (`random.seed(0)` and `np.random.seed(0)`).\n",
    "* **Hyperparameter ranges**: ranges (1â€“20, 2â€“10) are arbitrary; tune them to suit your problem.\n",
    "* **Stopping criteria**: instead of fixed `N_GENERATIONS`, you can stop if fitness doesnâ€™t improve for many generations.\n",
    "\n",
    "'''\n",
    "\n",
    "'''Excellent question, Saniya ğŸ‘ â€” cross-validation (especially **k-fold cross-validation**) is a *very important concept* in machine learning. Letâ€™s go step-by-step in **simple words** with an example.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **What is Cross-Validation?**\n",
    "\n",
    "Cross-validation is a **technique used to test how well a machine learning model will perform on unseen data**.\n",
    "\n",
    "ğŸ‘‰ In short, it helps us avoid **overfitting** (when a model performs well on training data but fails on new data).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ **Why do we need it?**\n",
    "\n",
    "If we only train and test our model on the same data, it might **memorize** patterns instead of learning them.\n",
    "So we need a smarter way to check:\n",
    "\n",
    "> â€œCan my model generalize well to data it has never seen?â€\n",
    "\n",
    "Thatâ€™s where **cross-validation** comes in.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ **How does it work (step-by-step)?**\n",
    "\n",
    "Letâ€™s take an example with **k = 5 folds** â€” i.e., **5-fold cross-validation**.\n",
    "\n",
    "Suppose you have **100 data samples**.\n",
    "\n",
    "1. **Split the data into 5 equal parts** (each with 20 samples).\n",
    "\n",
    "   ```\n",
    "   Fold1, Fold2, Fold3, Fold4, Fold5\n",
    "   ```\n",
    "\n",
    "2. **Train and test multiple times:**\n",
    "\n",
    "   * Round 1 â†’ Train on Fold2â€“Fold5, Test on Fold1\n",
    "   * Round 2 â†’ Train on Fold1, Fold3â€“Fold5, Test on Fold2\n",
    "   * Round 3 â†’ Train on Fold1â€“Fold2, Fold4â€“Fold5, Test on Fold3\n",
    "   * Round 4 â†’ Train on Fold1â€“Fold3, Fold5, Test on Fold4\n",
    "   * Round 5 â†’ Train on Fold1â€“Fold4, Test on Fold5\n",
    "\n",
    "3. Each round gives one **accuracy score**.\n",
    "\n",
    "4. Finally, take the **average of all 5 accuracies** â€” this gives a more **reliable estimate** of model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  **In your code:**\n",
    "\n",
    "You have this line:\n",
    "\n",
    "```python\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "```\n",
    "\n",
    "* `model`: your DecisionTreeClassifier\n",
    "* `X`: input features\n",
    "* `y`: target labels\n",
    "* `cv=5`: perform **5-fold cross-validation**\n",
    "\n",
    "ğŸ‘‰ What happens here:\n",
    "\n",
    "* Your dataset is split into 5 parts.\n",
    "* The model trains on 4 parts and tests on the remaining one.\n",
    "* This repeats 5 times (so each part gets to be the test set once).\n",
    "* The function returns 5 accuracy scores (one per round).\n",
    "\n",
    "Then you do:\n",
    "\n",
    "```python\n",
    "return scores.mean()\n",
    "```\n",
    "\n",
    "That means youâ€™re taking the **average accuracy** over all 5 folds.\n",
    "This average is used as the **fitness score** of that model configuration in your genetic algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š **Analogy (in simple words):**\n",
    "\n",
    "Imagine youâ€™re preparing for an exam.\n",
    "Instead of testing yourself on the same questions again and again (which you might memorize),\n",
    "you split the syllabus into 5 parts and test yourself on each part separately.\n",
    "Your average performance gives a **fair measure** of how prepared you are overall.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Benefits of Cross-Validation**\n",
    "\n",
    "* Gives a **more accurate** estimate of model performance.\n",
    "* Prevents **overfitting**.\n",
    "* Works well when the dataset is **small**.\n",
    "* Ensures every data point is used for both **training and testing** at least once.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show this concept visually using a **diagram** (like how data folds are rotated across rounds)? Itâ€™ll make it crystal clear.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
