{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee637875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Features (X) shape: (150, 4)\n",
      "Target (y) shape: (150,)\n",
      "First 5 rows of features (X):\n",
      "    sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n",
      "First 5 encoded target values (y):\n",
      " [0 0 0 0 0]\n",
      "Original species labels (first 5):\n",
      " 0    setosa\n",
      "1    setosa\n",
      "2    setosa\n",
      "3    setosa\n",
      "4    setosa\n",
      "Name: species, dtype: object\n",
      "Encoded target classes: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Starting Genetic Algorithm...\n",
      "Generation 1/10 - Best Fitness: 0.9733\n",
      "Generation 2/10 - Best Fitness: 0.9733\n",
      "Generation 3/10 - Best Fitness: 0.9733\n",
      "Generation 4/10 - Best Fitness: 0.9733\n",
      "Generation 5/10 - Best Fitness: 0.9733\n",
      "Generation 6/10 - Best Fitness: 0.9733\n",
      "Generation 7/10 - Best Fitness: 0.9733\n",
      "Generation 8/10 - Best Fitness: 0.9733\n",
      "Generation 9/10 - Best Fitness: 0.9733\n",
      "Generation 10/10 - Best Fitness: 0.9733\n",
      "\n",
      "Genetic Algorithm Finished.\n",
      "Best Hyperparameters: max_depth=3, min_samples_split=10\n",
      "Best Accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"SCOA_A4.csv\")\n",
    "\n",
    "# Identify feature and target columns\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y_categorical = df['species']\n",
    "\n",
    "# Encode the target column\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_categorical)\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(\"First 5 rows of features (X):\\n\", X.head())\n",
    "print(\"First 5 encoded target values (y):\\n\", y[:5])\n",
    "print(\"Original species labels (first 5):\\n\", y_categorical[:5])\n",
    "print(\"Encoded target classes:\", le.classes_)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# --- Genetic Algorithm Setup ---\n",
    "POP_SIZE = 20      # number of individuals\n",
    "N_GENERATIONS = 10 # iterations\n",
    "MUTATION_RATE = 0.2\n",
    "\n",
    "# Chromosome: [max_depth, min_samples_split]\n",
    "def create_chromosome():\n",
    "    return [random.randint(1, 20), random.randint(2, 10)]\n",
    "\n",
    "def fitness(chromosome):\n",
    "    max_depth, min_samples_split = chromosome\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split,\n",
    "                                   random_state=42) # Added random_state for reproducibility\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    return scores.mean()\n",
    "\n",
    "def selection(population, fitnesses):\n",
    "    # Select the best two individuals for crossover\n",
    "    idx = np.argsort(fitnesses)[-2:]\n",
    "    return [population[idx[0]], population[idx[1]]]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Perform single-point crossover\n",
    "    point = random.randint(0, len(parent1)-1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(chromosome):\n",
    "    # Apply mutation to max_depth\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[0] = random.randint(1, 20)\n",
    "    # Apply mutation to min_samples_split\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        chromosome[1] = random.randint(2, 10)\n",
    "    return chromosome\n",
    "\n",
    "# --- Run GA ---\n",
    "population = [create_chromosome() for _ in range(POP_SIZE)]\n",
    "\n",
    "print(\"\\nStarting Genetic Algorithm...\")\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fitnesses = [fitness(chromo) for chromo in population]\n",
    "    print(f\"Generation {gen+1}/{N_GENERATIONS} - Best Fitness: {max(fitnesses):.4f}\")\n",
    "\n",
    "    new_population = []\n",
    "    parents = selection(population, fitnesses)\n",
    "\n",
    "    # Generate new population through crossover and mutation\n",
    "    for _ in range(POP_SIZE // 2):\n",
    "        child1, child2 = crossover(parents[0], parents[1])\n",
    "        new_population.append(mutate(child1))\n",
    "        new_population.append(mutate(child2))\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "# Best result from the final population\n",
    "fitnesses = [fitness(chromo) for chromo in population]\n",
    "best_idx = np.argmax(fitnesses)\n",
    "best_hyperparameters = population[best_idx]\n",
    "best_accuracy = fitnesses[best_idx]\n",
    "\n",
    "print(\"\\nGenetic Algorithm Finished.\")\n",
    "print(f\"Best Hyperparameters: max_depth={best_hyperparameters[0]}, min_samples_split={best_hyperparameters[1]}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Step-by-step code explanation (simple language) — paste above your code\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) Import libraries\n",
    "#    - pandas: for reading and handling the dataset (tables, rows, columns).\n",
    "#    - sklearn.preprocessing.LabelEncoder: to convert text labels (species names)\n",
    "#      into numeric labels that machine learning models can use.\n",
    "#\n",
    "# 2) Load the dataset\n",
    "#    - df = pd.read_csv(\"SCOA_A4.csv\")\n",
    "#    - This reads the CSV file into a pandas DataFrame named df.\n",
    "#\n",
    "# 3) Identify features and target\n",
    "#    - X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "#      These four columns are used as the input features for the model.\n",
    "#    - y_categorical = df['species']\n",
    "#      This column (species names) is the target variable we want to predict.\n",
    "#\n",
    "# 4) Encode the target (LabelEncoder)\n",
    "#    - le = LabelEncoder()\n",
    "#    - y = le.fit_transform(y_categorical)\n",
    "#    - This converts species names (strings) into integers (e.g., 'setosa' -> 0).\n",
    "#      We keep the mapping in le.classes_; printing it shows which integer\n",
    "#      corresponds to which species name.\n",
    "#\n",
    "# 5) Print basic dataset info\n",
    "#    - The script prints confirmation and shapes:\n",
    "#      - \"Dataset loaded successfully.\"\n",
    "#      - \"Features (X) shape: (n_samples, 4)\"\n",
    "#      - \"Target (y) shape: (n_samples,)\"\n",
    "#      - Prints first rows so you can sanity-check the data.\n",
    "#\n",
    "# 6) Import additional libraries for model and evaluation\n",
    "#    - numpy, random: used for GA operations and array handling.\n",
    "#    - cross_val_score: to evaluate model performance using k-fold cross-validation.\n",
    "#    - DecisionTreeClassifier: the model used; its hyperparameters are tuned by the GA.\n",
    "#\n",
    "# 7) Genetic Algorithm (GA) setup\n",
    "#    - POP_SIZE = 20: number of candidate solutions (individuals) in each generation.\n",
    "#    - N_GENERATIONS = 10: number of GA iterations (how many times population evolves).\n",
    "#    - MUTATION_RATE = 0.2: probability of random changes in offspring.\n",
    "#    - Chromosome (representation): [max_depth, min_samples_split]\n",
    "#      - max_depth: how deep the decision tree may grow.\n",
    "#      - min_samples_split: minimum number of samples required to split an internal node.\n",
    "#\n",
    "# 8) create_chromosome()\n",
    "#    - Builds a random candidate hyperparameter pair:\n",
    "#      - max_depth randomly chosen from 1 to 20.\n",
    "#      - min_samples_split randomly chosen from 2 to 10.\n",
    "#\n",
    "# 9) fitness(chromosome)\n",
    "#    - For a given chromosome (hyperparams), this function:\n",
    "#      - Builds a DecisionTreeClassifier with those hyperparameters.\n",
    "#      - Runs cross_val_score(model, X, y, cv=5) to estimate the model's average accuracy.\n",
    "#      - Returns the mean cross-validation accuracy as the fitness value\n",
    "#        (higher is better).\n",
    "#\n",
    "# 10) selection(population, fitnesses)\n",
    "#    - Chooses two best individuals from the population (highest fitness) to act as parents.\n",
    "#\n",
    "# 11) crossover(parent1, parent2)\n",
    "#    - Single-point crossover:\n",
    "#      - Choose a cut point and swap parts to create two children.\n",
    "#      - Ensures children inherit hyperparameters from both parents.\n",
    "#\n",
    "# 12) mutate(chromosome)\n",
    "#    - Randomly modifies a chromosome with small probability (mutation).\n",
    "#      - Example: change max_depth or min_samples_split randomly.\n",
    "#    - Mutation helps maintain diversity and lets GA explore new regions.\n",
    "#\n",
    "# 13) Main GA loop\n",
    "#    - Initialize a random population of POP_SIZE chromosomes.\n",
    "#    - For each generation (repeat N_GENERATIONS times):\n",
    "#      - Evaluate fitness for each individual.\n",
    "#      - Select parents (best two).\n",
    "#      - Create offspring via crossover and mutation until new_population is filled.\n",
    "#      - Replace population with new_population.\n",
    "#\n",
    "# 14) Final result\n",
    "#    - After the GA finishes, compute fitnesses for final population.\n",
    "#    - Select the individual with the highest fitness.\n",
    "#    - Print the best hyperparameters and the best accuracy observed:\n",
    "#      - Example printed lines:\n",
    "#        \"Genetic Algorithm Finished.\"\n",
    "#        \"Best Hyperparameters: max_depth=..., min_samples_split=...\"\n",
    "#        \"Best Accuracy: 0.XXXX\"\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "# How to interpret the printed outputs (what to say in viva)\n",
    "# ------------------------------------------------------------------------\n",
    "# - \"Dataset loaded successfully.\" simply means the CSV was found and read.\n",
    "# - Features (X) shape: (n, 4) means there are n examples and 4 input features.\n",
    "# - Target (y) shape: (n,) means there are n label values.\n",
    "# - The printed \"first 5 rows\" and encoded labels let you check:\n",
    "#     - Values are reasonable (no missing garbage),\n",
    "#     - Label encoding mapping (le.classes_) shows original class names in order.\n",
    "# - During GA execution you typically won't see per-generation output unless printed,\n",
    "#   but the final \"Best Accuracy\" is the mean 5-fold CV accuracy for the best hyperparameters.\n",
    "# - Example interpretation: \"Best Accuracy: 0.92\" → the Decision Tree with the chosen\n",
    "#   hyperparameters achieved on average 92% accuracy across the 5 folds.\n",
    "#   It means the model predicted the correct species 92% of the time in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Detailed theory of Genetic Algorithms (GA) — comprehensive, viva-ready\n",
    "# Paste this as comments into a Jupyter cell or read aloud during viva.\n",
    "# -------------------------------------------------------------------------\n",
    "#\n",
    "# 1) What is a Genetic Algorithm (high-level)\n",
    "# -------------------------------------------\n",
    "# - A Genetic Algorithm (GA) is a population-based metaheuristic optimization\n",
    "#   method inspired by natural selection and genetics (biological evolution).\n",
    "# - It searches for good solutions to optimization/search problems by evolving\n",
    "#   a population of candidate solutions through repeated application of:\n",
    "#     Selection -> Crossover (recombination) -> Mutation.\n",
    "# - GAs are useful for problems with large, complex, discontinuous, or\n",
    "#   non-differentiable search spaces where gradient-based methods fail.\n",
    "#\n",
    "# 2) Historical note (one-liner)\n",
    "# ------------------------------\n",
    "# - Developed from ideas in the 1960s–1970s (notably John Holland); widely used\n",
    "#   since then across engineering, computer science, and applied sciences.\n",
    "#\n",
    "# 3) Core components of a GA\n",
    "# --------------------------\n",
    "# - Representation / Encoding:\n",
    "#   - How candidate solutions (individuals) are encoded as chromosomes (strings).\n",
    "#   - Common encodings:\n",
    "#       * Binary encoding: chromosome is bitstring (e.g., '101001').\n",
    "#       * Real-valued (floating point) encoding: genes are floats (good for continuous).\n",
    "#       * Permutation encoding: used for ordering problems (TSP); chromosome is a permutation.\n",
    "#       * Tree encoding: used in genetic programming (programs as trees).\n",
    "#\n",
    "# - Population:\n",
    "#   - A set of candidate solutions. Size is a hyperparameter (POP_SIZE).\n",
    "#   - Larger populations increase diversity but cost more evaluations.\n",
    "#\n",
    "# - Fitness Function:\n",
    "#   - Maps a candidate solution to a scalar that measures solution quality.\n",
    "#   - GA seeks to maximize (or minimize) fitness. For minimization, use negated loss.\n",
    "#   - Fitness must reflect true objective and constraints (often via penalties).\n",
    "#\n",
    "# - Selection Operators:\n",
    "#   - Choose parents for reproduction based on fitness (fitter individuals more likely).\n",
    "#   - Common methods:\n",
    "#       * Roulette-wheel selection (fitness-proportionate): probability ∝ fitness.\n",
    "#       * Ranking selection: individuals ranked; selection probability based on rank.\n",
    "#       * Tournament selection: pick k random individuals and select the best among them.\n",
    "#       * Stochastic universal sampling: similar to roulette but lower variance.\n",
    "#   - Tradeoffs: roulette can be dominated by very fit individuals; tournament is robust.\n",
    "#\n",
    "# - Crossover (Recombination):\n",
    "#   - Combine genetic material of two parents to create offspring.\n",
    "#   - Types:\n",
    "#       * Single-point crossover: split at one point, swap tails.\n",
    "#       * Two/multi-point crossover: split at multiple points.\n",
    "#       * Uniform crossover: each gene is chosen from either parent with a probability.\n",
    "#       * Arithmetic crossover (real-valued): offspring = α * parent1 + (1-α) * parent2.\n",
    "#       * Order crossover (permutation problems): preserves ordering constraints (e.g., OX for TSP).\n",
    "#   - Purpose: mix useful building blocks from parents to create potentially better offspring.\n",
    "#\n",
    "# - Mutation:\n",
    "#   - Randomly change genes to maintain diversity and explore new regions.\n",
    "#   - Types:\n",
    "#       * Bit-flip mutation (binary): flip bits with small probability.\n",
    "#       * Gaussian perturbation (real-valued): add N(0,σ) noise to genes.\n",
    "#       * Swap/inversion mutation (permutation): swap two positions or invert a subsequence.\n",
    "#   - Mutation rate is typically low (e.g., 0.001–0.1 depending on encoding).\n",
    "#\n",
    "# - Replacement / Survivor Selection:\n",
    "#   - Decide who survives to the next generation.\n",
    "#   - Strategies:\n",
    "#       * Generational replacement: all parents replaced by children.\n",
    "#       * Steady-state: only some individuals replaced each generation.\n",
    "#       * Elitism: best individual(s) always carried forward to avoid losing best found solution.\n",
    "#\n",
    "# 4) Pseudocode (standard GA)\n",
    "# ---------------------------\n",
    "# - population = initialize_random_population(POP_SIZE)\n",
    "# - evaluate fitness for all individuals\n",
    "# - while termination_condition_not_met:\n",
    "#     parents = select_parents(population, fitness)\n",
    "#     offspring = []\n",
    "#     while len(offspring) < POP_SIZE:\n",
    "#         p1, p2 = pick_two_parents(parents)\n",
    "#         if random() < CROSSOVER_RATE:\n",
    "#             c1, c2 = crossover(p1, p2)\n",
    "#         else:\n",
    "#             c1, c2 = p1.copy(), p2.copy()\n",
    "#         mutate(c1); mutate(c2)\n",
    "#         offspring.extend([c1, c2])\n",
    "#     population = replace(population, offspring, fitness)    # may include elitism\n",
    "#     evaluate fitness for any new individuals\n",
    "# - return best individual found\n",
    "\n",
    "#\n",
    "# 9) Parameter settings & practical guidelines\n",
    "# -------------------------------------------\n",
    "# - Population size (POP_SIZE):\n",
    "#   - Typical range: 20–500. Larger for complex or multi-modal problems.\n",
    "#   - Tradeoff: larger populations -> better coverage but more evaluations.\n",
    "# - Crossover rate (CR):\n",
    "#   - Often high: 0.6–1.0 for classical GAs (commonly 0.8–0.9).\n",
    "# - Mutation rate (MR):\n",
    "#   - Usually low for binary (0.001–0.01 per bit), higher for real-valued genes (0.01–0.1).\n",
    "# - Number of generations:\n",
    "#   - Depends on problem complexity and evaluation budget; use convergence criteria:\n",
    "#     * Max generations reached OR fitness improvement below threshold for N gens.\n",
    "# - Selection pressure:\n",
    "#   - Controls speed of convergence: high pressure -> fast convergence, risk of premature convergence.\n",
    "#   - Tune tournament size or ranking parameters to adjust pressure.\n",
    "# - Use elitism (1-5 individuals) to avoid losing best solutions.\n",
    "# - Always run GA multiple times with different random seeds — it is stochastic.\n",
    "#\n",
    "# 10) Convergence & stopping criteria\n",
    "# -----------------------------------\n",
    "# - Common choices:\n",
    "#   - Fixed number of generations or fitness evaluations.\n",
    "#   - No improvement in best fitness for T successive generations.\n",
    "#   - Achieve target fitness threshold.\n",
    "# - Note: reaching identical individuals in population often means convergence but not necessarily global optimum.\n",
    "#\n",
    "# 11) Computational complexity & evaluation cost\n",
    "# ----------------------------------------------\n",
    "# - The dominant cost is fitness evaluation (often involves simulations or ML training).\n",
    "# - Parallelization:\n",
    "#   - Fitness evaluations for individuals are independent — easy to parallelize.\n",
    "#   - Use parallel or distributed computing to reduce wall-clock time.\n",
    "#\n",
    "# 12) Strengths of GAs\n",
    "# --------------------\n",
    "# - Global search capability; less likely to be trapped by local minima compared to simple hill-climbers.\n",
    "# - Flexible: works with discrete, continuous, combinatorial, or mixed-variable problems.\n",
    "# - Does not require gradient information or convexity.\n",
    "# - Naturally parallelizable.\n",
    "#\n",
    "# 13) Weaknesses / pitfalls\n",
    "# -------------------------\n",
    "# - Computationally expensive if fitness is costly (many evaluations required).\n",
    "# - Stochastic: results vary between runs; needs multiple runs for reliability.\n",
    "# - Requires careful tuning of population size, mutation/crossover rates, and selection pressure.\n",
    "# - Poorly designed encoding or operators can make search ineffective (e.g., crossover destroys structure).\n",
    "# - Schema theory gives intuition but not strict guarantees of finding global optimum.\n",
    "#\n",
    "# 14) Common variants & hybrid approaches\n",
    "# ---------------------------------------\n",
    "# - Genetic programming (GP): evolves programs or expressions (tree-structured chromosomes).\n",
    "# - Evolution strategies (ES) and Differential Evolution (DE): related real-valued EAs with special mutation/recombination.\n",
    "# - Estimation of Distribution Algorithms (EDA): build probabilistic models of promising solutions.\n",
    "# - Memetic algorithms: hybridize GA with local search (hill-climbing) to refine offspring.\n",
    "# - Island models: multiple subpopulations occasionally exchange individuals (migration) to improve diversity.\n",
    "#\n",
    "# 15) Implementation tips (practical)\n",
    "# ----------------------------------\n",
    "# - Normalize/scale real-valued genes if needed; choose appropriate mutation distributions.\n",
    "# - Implement elitism to protect the best-so-far solution.\n",
    "# - Log best/mean/worst fitness per generation for diagnosing progress.\n",
    "# - Visualize convergence (plot best fitness vs generation).\n",
    "# - Use reproducible randomness (set random seeds) for debugging/demonstrations, but remember true performance needs multiple seeds.\n",
    "# - If fitness evaluation is expensive, consider surrogate models (approximate fitness with a cheaper model) or asynchronous parallel evaluation.\n",
    "# - Use tournament selection for simplicity and robustness.\n",
    "# - For permutation problems (TSP, scheduling), avoid standard crossover/mutation that break feasibility — use order-preserving operators.\n",
    "#\n",
    "# 16) Example use-cases (short)\n",
    "# -----------------------------\n",
    "# - Hyperparameter tuning (ML models), scheduling, routing (TSP), design optimization (engineering),\n",
    "#   feature selection, symbolic regression (GP), evolving neural network architectures (neuroevolution).\n",
    "#\n",
    "# 17) How to present GA in a viva (concise bullets)\n",
    "# ------------------------------------------------\n",
    "# - \"GA maintains a population of candidate solutions and uses selection, crossover, and mutation\n",
    "#    to explore and exploit the search space.\"\n",
    "# - \"Fitness guides selection; crossover recombines good partial solutions; mutation introduces novelty.\"\n",
    "# - \"Key hyperparameters: population size, crossover rate, mutation rate, number of generations.\"\n",
    "# - \"GAs are stochastic and require multiple runs; they are powerful for complex, non-differentiable problems.\"\n",
    "#\n",
    "# 18) Short mathematical notations (if asked)\n",
    "# -------------------------------------------\n",
    "# - Let P(t) be population at generation t.\n",
    "# - Fitness f : solution -> ℝ maps solutions to objective values.\n",
    "# - Expected number of copies of individual i in next gen under fitness-proportionate selection:\n",
    "#     E[copies_i] = (f_i / f̄) ,   where f̄ is mean fitness of population.\n",
    "# - Schema theorem (informal expected propagation):\n",
    "#     E[m(H,t+1)] ≥ m(H,t) * (f(H)/f̄) * (1 - p_c * (δ(H)/(l-1)) - o(H) * p_m)\n",
    "#   where m(H,t) = number of individuals matching schema H at generation t,\n",
    "#   f(H) = average fitness of schema H, δ(H) = defining length, o(H)=order of H,\n",
    "#   p_c = crossover probability, p_m = mutation probability. (This is conceptual; you can skip algebra.)\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
