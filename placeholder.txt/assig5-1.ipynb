{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbff861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded for AAL. Total records: 1259\n",
      "Training model...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5244 - loss: 0.6982\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5221 - loss: 0.6920\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5142 - loss: 0.6930\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5079 - loss: 0.6915\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5225 - loss: 0.6907\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5304 - loss: 0.6885\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5636 - loss: 0.6869\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5118 - loss: 0.6931\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5305 - loss: 0.6878\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5161 - loss: 0.6896\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5261 - loss: 0.6905\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5243 - loss: 0.6885\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5137 - loss: 0.6901\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5318 - loss: 0.6874\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5316 - loss: 0.6867\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5353 - loss: 0.6882\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5349 - loss: 0.6878\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5366 - loss: 0.6879\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5252 - loss: 0.6890\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5430 - loss: 0.6868\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5478 - loss: 0.6869\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5054 - loss: 0.6887\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5383 - loss: 0.6879\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5167 - loss: 0.6870\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5280 - loss: 0.6878\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5245 - loss: 0.6858\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5178 - loss: 0.6856\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5149 - loss: 0.6880\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5287 - loss: 0.6871\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5392 - loss: 0.6867\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5142 - loss: 0.6880\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5483 - loss: 0.6847\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5397 - loss: 0.6849\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5614 - loss: 0.6843\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5388 - loss: 0.6871\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 0.6858\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5247 - loss: 0.6880\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5478 - loss: 0.6853\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5629 - loss: 0.6859\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5222 - loss: 0.6863\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5322 - loss: 0.6843\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5481 - loss: 0.6855\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5252 - loss: 0.6859\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5373 - loss: 0.6868\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5468 - loss: 0.6847\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5304 - loss: 0.6873\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5323 - loss: 0.6848\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5459 - loss: 0.6845\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5500 - loss: 0.6843\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5432 - loss: 0.6862\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5225 - loss: 0.6862\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5078 - loss: 0.6864\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5431 - loss: 0.6840\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5363 - loss: 0.6832\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5364 - loss: 0.6823\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5508 - loss: 0.6851\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5365 - loss: 0.6863\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5325 - loss: 0.6877\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5327 - loss: 0.6839\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5207 - loss: 0.6857\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5343 - loss: 0.6854\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5525 - loss: 0.6823\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5418 - loss: 0.6844\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5359 - loss: 0.6831\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5363 - loss: 0.6825\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5356 - loss: 0.6824\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5553 - loss: 0.6822\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5370 - loss: 0.6844\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5171 - loss: 0.6850\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5323 - loss: 0.6838\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5280 - loss: 0.6848\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5439 - loss: 0.6831\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5294 - loss: 0.6838\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 0.6859\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5430 - loss: 0.6827\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5286 - loss: 0.6828\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5386 - loss: 0.6816\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5327 - loss: 0.6853\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5728 - loss: 0.6795\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5551 - loss: 0.6820\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5662 - loss: 0.6814\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5559 - loss: 0.6820\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5211 - loss: 0.6828\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5428 - loss: 0.6826\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5279 - loss: 0.6826\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5638 - loss: 0.6826\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5397 - loss: 0.6838\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5398 - loss: 0.6837\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5255 - loss: 0.6846\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5464 - loss: 0.6837\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5128 - loss: 0.6844\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5243 - loss: 0.6826\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5410 - loss: 0.6830\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5352 - loss: 0.6854\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5343 - loss: 0.6824\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5363 - loss: 0.6824\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5431 - loss: 0.6836\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5543 - loss: 0.6801\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5373 - loss: 0.6815\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5286 - loss: 0.6840\n",
      "Training complete.\n",
      "\n",
      "--- Model Evaluation for AAL ---\n",
      "Test Accuracy: 0.5238\n",
      "Confusion Matrix (True Negatives, False Positives, False Negatives, True Positives):\n",
      "[[98 25]\n",
      " [95 34]]\n",
      "\n",
      "- True Positives (Correctly predicted UP): 34\n",
      "- True Negatives (Correctly predicted DOWN): 98\n",
      "- False Positives (Predicted UP, but went DOWN): 25\n",
      "- False Negatives (Predicted DOWN, but went UP): 95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "\n",
    "# --- Configuration ---\n",
    "# File path for the uploaded dataset\n",
    "FILE_PATH = 'SCOA_A5.csv'\n",
    "# The CSV contains multiple stocks. We filter for one to predict future movement.\n",
    "STOCK_TICKER = 'AAL'\n",
    "\n",
    "# Step 1: Load stock data from CSV\n",
    "try:\n",
    "    data = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {FILE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# Filter data for the selected stock ticker\n",
    "df = data[data['Name'] == STOCK_TICKER].copy()\n",
    "\n",
    "# Ensure the data is sorted by date for correct shifting\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Create the prediction target: 1 if the next day's close is higher, 0 otherwise.\n",
    "# Note: The column names are in lowercase in the CSV (e.g., 'close').\n",
    "df['Target'] = np.where(df['close'].shift(-1) > df['close'], 1, 0)\n",
    "\n",
    "# Drop the last row, as its target cannot be calculated (Target will be NaN)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Data loaded for {STOCK_TICKER}. Total records: {len(df)}\")\n",
    "\n",
    "\n",
    "# Step 2: Feature selection and scaling\n",
    "# Use the same features as the original script, adjusted for lowercase column names.\n",
    "features = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "y = df['Target'].values.astype(np.int32) # Ensure target is integer type\n",
    "\n",
    "# Step 3: Train-test split\n",
    "# Using shuffle=False is critical for time-series data to maintain chronological order.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Step 4: Build and train ANN model\n",
    "# Input dimension is 5 (open, high, low, close, volume)\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training model...\")\n",
    "# Training with reduced verbosity for cleaner output\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Step 5: Evaluate model\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation for {STOCK_TICKER} ---\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix (True Negatives, False Positives, False Negatives, True Positives):\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Interpretation of the confusion matrix (optional but helpful)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "print(f\"\\n- True Positives (Correctly predicted UP): {tp}\")\n",
    "print(f\"- True Negatives (Correctly predicted DOWN): {tn}\")\n",
    "print(f\"- False Positives (Predicted UP, but went DOWN): {fp}\")\n",
    "print(f\"- False Negatives (Predicted DOWN, but went UP): {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# ðŸ“˜ PROJECT: Stock Movement Prediction using Artificial Neural Network (ANN)\n",
    "# -------------------------------------------------------------\n",
    "# This program predicts whether a stockâ€™s next day closing price will\n",
    "# go UP (1) or DOWN (0) using a feedforward Artificial Neural Network.\n",
    "# -------------------------------------------------------------\n",
    "# Libraries used:\n",
    "#  - numpy, pandas â†’ data handling\n",
    "#  - sklearn â†’ scaling, splitting, evaluation metrics\n",
    "#  - keras (Sequential, Dense, Dropout) â†’ building ANN model\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# --- Configuration ---\n",
    "# File path for dataset and ticker selection\n",
    "FILE_PATH = 'SCOA_A5.csv'      # dataset file\n",
    "STOCK_TICKER = 'AAL'           # stock symbol to filter\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 1: LOAD AND PREPARE THE DATA\n",
    "# -------------------------------------------------------------\n",
    "try:\n",
    "    data = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {FILE_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# Filter data for selected stock ticker\n",
    "df = data[data['Name'] == STOCK_TICKER].copy()\n",
    "\n",
    "# Convert 'date' column to datetime type and sort by date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Create the target variable:\n",
    "# If next day's close price > today's close â†’ 1 (UP), else 0 (DOWN)\n",
    "df['Target'] = np.where(df['close'].shift(-1) > df['close'], 1, 0)\n",
    "\n",
    "# Drop the last row because its \"next day\" value does not exist (NaN)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Data loaded for {STOCK_TICKER}. Total records: {len(df)}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 2: FEATURE SELECTION AND SCALING\n",
    "# -------------------------------------------------------------\n",
    "# Select features: open, high, low, close, and volume\n",
    "# These are numeric features that influence price movement.\n",
    "features = df[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "# MinMaxScaler scales all feature values between 0 and 1.\n",
    "# This helps the neural network train faster and avoids dominance\n",
    "# of larger numeric features (e.g., volume).\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# Target (output labels)\n",
    "y = df['Target'].values.astype(np.int32)  # Convert to integer 0/1\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 3: TRAIN-TEST SPLIT\n",
    "# -------------------------------------------------------------\n",
    "# We split 80% data for training and 20% for testing.\n",
    "# Note: shuffle=False is important for time-series data\n",
    "# so that we do not mix past and future data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 4: BUILD ANN MODEL\n",
    "# -------------------------------------------------------------\n",
    "# The ANN (Artificial Neural Network) is a collection of layers:\n",
    "#  - Input layer (5 neurons = 5 features)\n",
    "#  - Hidden layers (ReLU activations for non-linearity)\n",
    "#  - Dropout layers to prevent overfitting\n",
    "#  - Output layer (Sigmoid for binary classification)\n",
    "#\n",
    "# Theory:\n",
    "#  Each neuron computes: z = w*x + b\n",
    "#  Activation functions:\n",
    "#     ReLU: f(x) = max(0, x)\n",
    "#     Sigmoid: Ïƒ(x) = 1 / (1 + e^-x)\n",
    "#\n",
    "#  Loss function: Binary Cross-Entropy\n",
    "#     L = - [y*log(p) + (1-y)*log(1-p)]\n",
    "#  Optimizer: Adam (adaptive gradient method)\n",
    "#  Backpropagation adjusts weights to minimize loss.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Hidden layer 1\n",
    "model.add(Dropout(0.2))                                               # Drop 20% neurons randomly\n",
    "model.add(Dense(64, activation='relu'))                               # Hidden layer 2\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))                               # Hidden layer 3\n",
    "model.add(Dense(1, activation='sigmoid'))                             # Output layer (0-1 probability)\n",
    "\n",
    "# Compile model:\n",
    "# - binary_crossentropy: suitable for binary classification\n",
    "# - adam: adaptive optimizer (fast convergence)\n",
    "# - accuracy: metric to evaluate performance\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 5: TRAIN THE MODEL\n",
    "# -------------------------------------------------------------\n",
    "# Model learns weights by minimizing loss using backpropagation.\n",
    "# epochs â†’ how many passes over the entire dataset\n",
    "# batch_size â†’ how many samples processed before updating weights\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# STEP 6: MODEL EVALUATION\n",
    "# -------------------------------------------------------------\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Convert probabilities into binary class (0 or 1)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "# Accuracy = (TP + TN) / Total\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix gives TP, TN, FP, FN counts\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation for {STOCK_TICKER} ---\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# INTERPRET CONFUSION MATRIX\n",
    "# -------------------------------------------------------------\n",
    "# Confusion matrix layout: [[TN, FP], [FN, TP]]\n",
    "#  - True Positive (TP): predicted UP, actually UP\n",
    "#  - True Negative (TN): predicted DOWN, actually DOWN\n",
    "#  - False Positive (FP): predicted UP, actually DOWN\n",
    "#  - False Negative (FN): predicted DOWN, actually UP\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "print(f\"\\n- True Positives (Correctly predicted UP): {tp}\")\n",
    "print(f\"- True Negatives (Correctly predicted DOWN): {tn}\")\n",
    "print(f\"- False Positives (Predicted UP, but went DOWN): {fp}\")\n",
    "print(f\"- False Negatives (Predicted DOWN, but went UP): {fn}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# ADDITIONAL THEORY (for viva preparation)\n",
    "# -------------------------------------------------------------\n",
    "# âž¤ Artificial Neural Network (ANN)\n",
    "#    - Mimics the human brainâ€™s neuron connections.\n",
    "#    - Composed of layers of interconnected nodes (neurons).\n",
    "#    - Each neuron computes a weighted sum of inputs plus bias,\n",
    "#      applies an activation function, and passes output forward.\n",
    "#\n",
    "# âž¤ Learning Process\n",
    "#    - Forward Pass: compute predicted outputs.\n",
    "#    - Loss Calculation: measure error between predicted and actual.\n",
    "#    - Backpropagation: propagate error backward, update weights.\n",
    "#    - Optimizer (Adam): updates weights using gradient descent.\n",
    "#\n",
    "# âž¤ Why Scaling?\n",
    "#    - Ensures all features have equal influence.\n",
    "#    - Improves gradient descent convergence speed.\n",
    "#\n",
    "# âž¤ Why Dropout?\n",
    "#    - Prevents overfitting by randomly deactivating neurons.\n",
    "#\n",
    "# âž¤ Why Sigmoid Output?\n",
    "#    - Converts output into probability between 0 and 1.\n",
    "#\n",
    "# âž¤ Why Binary Cross-Entropy Loss?\n",
    "#    - Measures how close predicted probabilities are to true labels.\n",
    "#\n",
    "# âž¤ Common Improvements:\n",
    "#    - Fit scaler only on training data (avoid data leakage).\n",
    "#    - Use TimeSeriesSplit for better validation.\n",
    "#    - Add more features like returns, moving averages, RSI, etc.\n",
    "#    - Try LSTM or GRU for sequential (time-dependent) data.\n",
    "#    - Use EarlyStopping to avoid overtraining.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# END OF SCRIPT\n",
    "# -------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efe377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# INTERPRETATION OF THE TRAINING LOG & EVALUATION (paste as comments)\n",
    "# -------------------------------------------------------------\n",
    "# Training log summary:\n",
    "#  - Training accuracy hovers around ~0.50â€“0.57 and the training loss stays ~0.68â€“0.69.\n",
    "#  - Note: binary cross-entropy for random guessing (p=0.5) â‰ˆ 0.6931 â€” so loss â‰ˆ 0.69 means\n",
    "#    the model is often predicting probabilities near 0.5 (i.e. not confident).\n",
    "#  - The accuracy does not show steady improvement across epochs â€” it fluctuates.\n",
    "#\n",
    "# Test set results (confusion matrix given):\n",
    "#  [[TN, FP],\n",
    "#   [FN, TP]] = [[91, 32],\n",
    "#                [76, 53]]\n",
    "#  - Total samples = 91 + 32 + 76 + 53 = 252\n",
    "#  - Test Accuracy = (TP + TN) / Total = (53 + 91) / 252 = 0.5714 (57.14%)\n",
    "#\n",
    "# Other useful metrics (from confusion matrix):\n",
    "#  - Precision (for predicted UP) = TP / (TP + FP) = 53 / 85  â‰ˆ 0.6235\n",
    "#    -> When model predicts UP, it's correct ~62.4% of the time.\n",
    "#  - Recall / Sensitivity (for actual UP) = TP / (TP + FN) = 53 / 129 â‰ˆ 0.4109\n",
    "#    -> The model finds only ~41.1% of actual UP days (many false negatives).\n",
    "#  - F1-score â‰ˆ 0.4953 (harmonic mean of precision and recall).\n",
    "#\n",
    "# Quick verbal interpretation you can say in viva:\n",
    "#  - \"Training loss near 0.69 means the network's outputs are close to 0.5 (uninformative).\n",
    "#    Accuracy around 50â€“57% is only slightly better than random guessing for a balanced set.\n",
    "#    Precision is okay (~62%) but recall is low (~41%), meaning the model is conservative\n",
    "#    about predicting 'UP' and misses many real UP days (high false negatives).\"\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# WHY ACCURACY / PERFORMANCE IS LOW â€” IMPLEMENTATION & MODEL ISSUES\n",
    "# (short, viva-friendly comment bullets)\n",
    "# -------------------------------------------------------------\n",
    "# 1) Target problem is very noisy:\n",
    "#    - Next-day up/down is inherently noisy and often close to random because markets\n",
    "#      react to new information. Simple MLP on raw price/volume may not capture the signal.\n",
    "#\n",
    "# 2) Model outputs near 0.5 (loss â‰ˆ 0.69) => little learning\n",
    "#    - Either the model cannot find a useful pattern in the given features, or label noise\n",
    "#      / bad features make the mapping unlearnable.\n",
    "#\n",
    "# 3) Weak / inappropriate features:\n",
    "#    - Using only [open, high, low, close, volume] for a single-day sample ignores temporal\n",
    "#      context (momentum, trends, lags). The model has no memory of past days.\n",
    "#\n",
    "# 4) Architecture mismatch for time-series:\n",
    "#    - Plain feedforward MLP treats each day independently and cannot model sequences.\n",
    "#      Time-dependencies are vital in stock data.\n",
    "#\n",
    "# 5) Possible data issues:\n",
    "#    - Label noise, missing values, incorrect preprocessing or scaling mistakes (e.g., fitting\n",
    "#      scaler on whole dataset leaks info or produces unstable transforms).\n",
    "#\n",
    "# 6) Insufficient feature engineering:\n",
    "#    - No returns, no lagged features, no technical indicators (moving averages, RSI, etc.).\n",
    "#\n",
    "# 7) Poor training regime:\n",
    "#    - No validation set / early stopping -> may be training longer than necessary or not\n",
    "#      tuning hyperparameters. Also no reproducibility (random seeds).\n",
    "#\n",
    "# 8) Evaluation metric choice:\n",
    "#    - Accuracy alone can be misleading. The model shows asymmetric errors (low recall).\n",
    "#\n",
    "# 9) Overfitting vs underfitting:\n",
    "#    - The near-constant high loss suggests underfitting or that the model can't extract signal.\n",
    "#      Dropout may be excessive given the small feature set, or the model is not the right type.\n",
    "#\n",
    "# 10) Class threshold and calibration:\n",
    "#    - Using 0.5 threshold may not be optimal â€” probabilities might be poorly calibrated.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# SUGGESTED FIXES / IMPROVEMENTS (prioritised & practical â€” paste into code as comments)\n",
    "# -------------------------------------------------------------\n",
    "# A. Quick fixes to check first\n",
    "#    1. Check scaler leakage:\n",
    "#       - Fit scaler on X_train only, then transform X_test. (Avoid fitting on whole X before split.)\n",
    "#         Example:\n",
    "#           X_train_raw, X_test_raw, y_train, y_test = train_test_split(features, y, test_size=0.2, shuffle=False)\n",
    "#           scaler = MinMaxScaler()\n",
    "#           X_train = scaler.fit_transform(X_train_raw)\n",
    "#           X_test  = scaler.transform(X_test_raw)\n",
    "#\n",
    "#    2. Add a proper validation set / use TimeSeriesSplit for hyperparameter tuning.\n",
    "#    3. Add EarlyStopping callback (monitor val_loss) to stop training when model stops improving.\n",
    "#\n",
    "# B. Feature engineering (high impact)\n",
    "#    1. Use returns instead of raw prices:\n",
    "#         return_t = (close_t - close_{t-1}) / close_{t-1}\n",
    "#    2. Add lag features (window of past N days): e.g., last 5â€“20 days returns or closes.\n",
    "#    3. Add technical indicators: moving averages, momentum, RSI, MACD, Bollinger Bands, volatility.\n",
    "#    4. Use volume-normalized features or log(volume) if skewed.\n",
    "#\n",
    "# C. Model / architecture changes\n",
    "#    1. Use sequence models when you include temporal windows:\n",
    "#       - LSTM / GRU / 1D-CNN or Transformer for time-series data. These capture temporal patterns.\n",
    "#       - Example approach: for each sample, feed a matrix shape (window_size, features).\n",
    "#    2. If staying with MLP, provide windowed features (flatten last N days) rather than single day.\n",
    "#\n",
    "# D. Training & evaluation improvements\n",
    "#    1. Use class weights or resampling if classes are imbalanced.\n",
    "#    2. Try different thresholds and evaluate ROC-AUC, precision/recall curves, F1 â€” not only accuracy.\n",
    "#    3. Perform hyperparameter search (layers, neurons, learning rate, batch_size).\n",
    "#    4. Use model checkpointing and save best model based on validation metric.\n",
    "#    5. Set random seeds (numpy, tensorflow) for reproducibility.\n",
    "#\n",
    "# E. Robustness & real-world considerations\n",
    "#    1. Use walk-forward validation (rolling window) to simulate real trading conditions.\n",
    "#    2. Test across market regimes (bull, bear) and ensure model generalizes.\n",
    "#    3. Work with risk-adjusted metrics or strategy backtest (include transaction costs).\n",
    "#\n",
    "# F. Baselines & diagnostics\n",
    "#    1. Always compare to simple baselines:\n",
    "#       - Always predict majority class\n",
    "#       - Logistic regression on engineered features\n",
    "#       - Decision tree / random forest\n",
    "#    2. If a simple baseline matches your ANN, the ANN is not adding value â€” go back to feature engineering.\n",
    "#\n",
    "# G. Calibration & threshold tuning\n",
    "#    - Compute predicted probabilities on validation set, plot ROC, choose threshold that balances precision/recall for your objective.\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# CONCISE VIVA LINES (say these if asked \"Why is accuracy low and what will you do?\")\n",
    "# -------------------------------------------------------------\n",
    "# - \"Accuracy is low because next-day price direction is noisy and the model currently has weak features \n",
    "#    (single-day raw prices) and no temporal context. Training loss â‰ˆ0.69 indicates the network often outputs \n",
    "#    probabilities near 0.5 (uninformative).\"\n",
    "# - \"I will fix preprocessing (fit scaler on train), add lagged features and indicators, use a time-series\n",
    "#    model (LSTM or windowed MLP), introduce proper validation (TimeSeriesSplit), and tune hyperparameters.\"\n",
    "#\n",
    "# -------------------------------------------------------------\n",
    "# END OF COMMENT BLOCK\n",
    "# -------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
